---
title: "First steps: filtering, visualisation, and basic normalisation"
teaching: 30
exercises: 0
---

:::::::::::::::::::::::::::::::::::::: questions 

- FIXME

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: objectives

- FIXME

::::::::::::::::::::::::::::::::::::::::::::::::


```{r, echo=FALSE, include=FALSE}
options(digits=3)
library(ggplot2)
library(ggrepel)
library(ggfortify)
library(scales)
library(pheatmap)
library(matrixStats)
library(openxlsx)
library(limma)
library(edgeR)
load(file="data/01-introduction.RData")
```


## Filtering

So, assuming your counts table has biological replicates in each group (and any technical replicates have been taken care of), the first thing we need to do is *filtering*. More precisely, removing genes with little to no expression, because the former add "noise" while the latter aren't even relevant. There are various approaches to filtering, but one of the simplest is to use the `limma` function `filterByExpression`:
```{r}
keep <- filterByExpr(dge, group=group) # logical vector indicating which rows of dge to keep
table(keep) # 32069 genes will be removed; 14009 genes will be kept

dge <- dge[keep,,keep.lib.size=FALSE]
head(dge$counts) # compare to pre-filtered data to see the difference
```
Roughly speaking, this strategy keeps genes that have a reasonable number of counts in a worthwhile number samples, and removes them otherwise (see function help for precise details).

Following filtering, you should check the *library size* of each sample. The library size of a sample is simply the sum of the counts in that library, i.e.
$$
\begin{align*}
\text{Library size for sample}\ j &= \sum_{i} Y_{ij} \\
& = \sum_{\substack{\text{all genes}}} \text{counts for sample}\ j
\end{align*}
$$
where $Y_{ij}$ is the count for gene $i$ in sample $j$. Here's a plot of the library sizes for our data:
```{r}
lib.size.millons <- colSums(dge$counts) / 1e6

barplot(lib.size.millons, las=2, cex.names=0.8, main="Library size (millons)")
abline(h=20, lwd=2, col="red2")
```

Library size is an important quantity. One reason to check it is because bigger is usually better: 5 million or less (not that good); 10-15 million (better); 20+ million (much better). This is because (1) it's a limited resource that needs to be "shared" between all the genes (in proportion to their expression); and (2) bigger counts generally means better statistical properties (e.g. power to detect DE). Another reason to check is because it will help signpost what tools you should use to analyse your data (more about this later).

## Visualisation

The next thing to do is *visualise* your data. This allows you to (1) see the landscape of biological differences between experimental groups, and (2) assess whether the data is affected by technical variation. Detecting technical variation is particularly important because its presence leads to missed and/or false discoveries, i.e. rubbish DE results. Thus, this step is crucial: analysing RNA-seq data without visualisation is literally "flying blind"!

There are many visualisation methods. Each has different advantages and show different aspects of the data---there is no single perfect visualisation, so use several! Here we'll discuss three: 

1. *PCA plots*. These give a special low dimensional representation of our (very) high dimensional data (i.e. where we've taken thousands of measurements on each sample). A ``good" plot looks like this: (1) replicates *cluster*; and (2) there's some *separation* of biological groups.

2. *Boxplots*. These are very simple: you take all the expression measurements in each sample and dump them into a boxplot, one boxplot for each sample. A ``good" set of plots looks like this: the boxplots are lined up, without much movement between them.

3. *RLE plots*. These are the same as boxplots but with one extra (crucial) step: subtracting the row median from each row before dumping the observations into boxplots. This additional step removes the between-gene variation, making the plot more sensitive than boxplots for detecting technical variation. A ``good" plot looks like this: the boxplot medians are lined up around zero and the boxes are roughly the same size.

If any of these plots don't look ``good", in the ways described, this indicates the presence of technical variation. 

Let's make each plot then discuss them:
```{r, fig.width=5, fig.height=4}
# log transform counts, note: 1/2 offset to avoid log transforming zeros
y <- log2(dge$counts + 1/2)

# PCA plot (nice ggplot version, potentially for publication)
pca <- prcomp(t(y), center=TRUE) 
autoplot(pca, data.frame(group=group), col='group', size=4) + 
  theme_classic() +
  ggtitle("PCA plot")
```
```{r, fig.width=9, fig.height=4.5}
# boxplot and RLE plot (simple base R versions; they're generally not for publication)
par(mfcol=c(1,2))
cols.group <- group
levels(cols.group) <- hue_pal()(4)

boxplot(y, col=as.character(cols.group), outline=FALSE, las=2, cex.axis=0.6, main="Boxplots")

RLE <- y - rowMedians(y)
boxplot(RLE, col=as.character(cols.group), outline=FALSE, las=2, cex.axis=0.6, main="RLE plot")
abline(h=0, col="grey20", lwd=3, lty=1)
```

Note that we made these plots with log transformed counts, not the counts themselves (the reason for this is technical: the log transformation roughly removes the strong mean-variance relationship inherent in RNA-seq data, something we don't want driving our visualisations). 

For our data, the PCA plot looks okay; the replicates cluster reasonably well and there's separation of biological groups. The boxplots and RLE plot, on the other hand, tell a different story (which is why you should visualise your data multiple ways!). The boxplots are wobbling around a little, which is a problem. Indeed, the RLE plot reveals things are far from ideal. The boxplots and RLE plot clearly indicate the presence of technical variation which needs to be removed if we're to obtain non-rubbish results. This is the topic we turn to next.

## Basic normalisation

Unfortunately, when doing RNA-seq analysis we can't simply compare counts between samples because there's almost always technical differences between them. To make samples properly comparable, the data needs to be *normalised*: we need to remove or adjust for technical differences between samples, leaving only biological differences (the thing we're interested in!). Failing to successfully normalise your data can lead to significant missed and/or false discoveries (rubbish results).

Two important technical differences between samples are (1) library size differences and (2) compositional differences:

1. *Library size differences*. To see the problem here look at Gene 3 in the made up table of counts:



![](fig/table-of-counts.png){width=70%}



Does Gene 3 show a biological or technical difference between the two groups? Looks like a biological difference, but what if I told you the library sizes for group B were 3 times bigger? In that case, the difference looks more like an artifact of library size rather than a true biological difference. Let's look at the problem another way:



![](fig/differences-lib-size.png){width=70%}



It should be clear from this that differences resulting from different library sizes can be mistaken for biological differences. One simple way to (roughly) normalise for library size differences is to calculate *counts-per-million (CPM)*. If we again let $Y_{ij}$ be the count for gene $i$ in sample $j$, and let $N_j$ be the library size for sample $j$, then the CPM value for gene $i$ in sample $j$ is given by
$$
\text{CPM}_{ij} = \frac{Y_{ij}}{N_j} \times 10^6 = \frac{\text{Count}}{\text{Library size}} \times 10^6,
$$
Because CPM values are (scaled) *proportions*, i.e. "Count / Library size", not absolute counts, the issue of library size differences is no longer relevant.

2. *Compositional differences*. For each sample, the probability of sequencing a transcript is proportional to its abundance---this is the basis of how RNA-seq works (of course!). To see the potential problem this fact creates, look at this figure:



![](fig/differences-composition.png){width=70%}



These two samples are the same (they have the same number of red, yellow, blue, and green transcripts) except for the black transcript: sample A has many more black transcripts. This difference in composition means that even if the library sizes are the same, comparing raw counts still gives the wrong impression. One popular approach for normalising out these differences is the `TMM` method (from the `edgeR` package).

Here's how to adjust for library size and compositional differences in our example:
```{r}
dge <- calcNormFactors(dge, method="TMM") # adjustment factors for composition; stored in our container for later use
CPM <- cpm(dge) # adjustment factors for composition incorporated when calculating cpm values
head(CPM) # compare to the raw counts
```

Before we can proceed, however, we need to do something *very important*: we need to *check* that the normalisation methodology we've applied has actually worked! One way of doing this is through visualisation. You want to see that the normalisation has (1) removed technical (unwanted) variation, while (2) retaining (i.e. not removing) the biological differences of interest. For example, here's what our data looks like after normalising for library size and composition:
```{r, fig.width=5, fig.height=4}
# log transform adjusted CPM values
y <- cpm(dge, log=TRUE)

# PCA plot (nice ggplot version, potentially for publication)
pca <- prcomp(t(y), center=TRUE) 
autoplot(pca, data.frame(group=group), col='group', size=4) + 
  theme_classic() +
  ggtitle("PCA plot")
```
```{r, fig.width=9, fig.height=4.5}
# boxplot and RLE plot (simple base R versions; they're generally not for publication)
par(mfcol=c(1,2))
cols.group <- group
levels(cols.group) <- hue_pal()(4)

boxplot(y, col=as.character(cols.group), outline=FALSE, las=2, cex.axis=0.6, main="Boxplots")

RLE <- y - rowMedians(y)
boxplot(RLE, col=as.character(cols.group), outline=FALSE, las=2, cex.axis=0.6, main="RLE plot")
abline(h=0, col="grey20", lwd=3, lty=1)
```

Note that we can now use the cpm function to calculate logged data. The PCA plot looks better: the replicates are clustered more tightly (technical variation has been removed), and the biological groups are more separated (biological differences between groups are maintained, actually improved). Moreover, the boxplots and RLE plot are looking much better: the boxplots are lined up, and the RLE plot medians are centered on zero and the boxes are roughly similar in size.

WARNING!...library size and composition differences arenâ€™t the only kinds of technical differences that can arise between samples:

- Another way they can arise is when different samples are prepared differently, e.g.

  - with different protocols, or
  - in different labs, or
  - on different dates, or
  - by different people, or
  - etc.


  
  These differences can lead to so-called "batch effects". For example, consider these PCA plots of a real data set:



![](fig/batch-effect.png){width=80%}



These are both the "same" PCA plot, but one is coloured by biological condition and the other is coloured by batch. The major factor driving separation (along PC1) is batch, not biological condition; consequently, replicates are not clustering by group, and the groups aren't separating. Batch effects like these can only be corrected with more sophisticated analysis approaches, a topic outside the scope of this workshop. 

- Moreover, batch effects aren't the only way problems can arise (beyond library size and composition). For example, consider this PCA plot of another real data set (coloured by biological condition):



![](fig/bad-PCA.png){width=40%}



Again, the replicates aren't really clustering and the groups aren't really separating. The problem here, however, isn't batch effects---it's something much more subtle.

The bottom line is this: if, after normalising for library size and composition, your visualisations don't look reasonably "good", in the ways discussed above, the results you'll get from proceeding with a DE analysis will likely contain missed and/or false discoveries---you'll likely get rubbish results.

```{r, include=FALSE}
save.image(file="data/02-filtering-visualisation-normalisation.RData")
```

:::::::::::::::::::::::::::::::::::::: keypoints

- FIXME

::::::::::::::::::::::::::::::::::::::
